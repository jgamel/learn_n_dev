{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pyspark_perm_mount.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNgyx1S/uHenDJEL/4L8BBF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jgamel/learn_n_dev/blob/google_colabs_library_mount/pyspark_perm_mount.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install pyspark Permanently on gdrive"
      ],
      "metadata": {
        "id": "l0ta5OeHiZPM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXag8nkuiQZa",
        "outputId": "bfd9598d-3be9-4bc8-8e07-ff5bbc92152e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "import os, sys \n",
        "#to be able to interact with Google Drive's operating system\n",
        "from google.colab import drive \n",
        "#drive is a module that allows us use Python to interact with google drive\n",
        "drive.mount('/content/gdrive') \n",
        "#mounting google drive allows us to work with its contents\n",
        "nb_path = '/content/notebooks'\n",
        "os.symlink('/content/gdrive/My Drive/Colab Notebooks', nb_path)\n",
        "sys.path.insert(0, nb_path)  # or append(nb_path)\n",
        "#The last three lines are what changes the path of the file."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PySpark is an interface for Apache Spark in Python. It not only allows you to write Spark applications using Python APIs, but also provides the PySpark shell for interactively analyzing your data in a distributed environment."
      ],
      "metadata": {
        "id": "EpElPBTtimVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --target=$nb_path pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rU7UGZ6lio1q",
        "outputId": "00322151-921e-4dc3-e269-e9e60540499b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.4 MB 34 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.3\n",
            "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 64.6 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=3fc907ede143b98cb2cf4408a49eecb43b50d5a7f55303be06d9a0791c017848\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/f5/07/7cd8017084dce4e93e84e92efd1e1d5334db05f2e83bcef74f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n",
            "\u001b[33mWARNING: Target directory /content/notebooks/py4j-0.10.9.3.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
            "\u001b[33mWARNING: Target directory /content/notebooks/pyspark already exists. Specify --upgrade to force replacement.\u001b[0m\n",
            "\u001b[33mWARNING: Target directory /content/notebooks/py4j already exists. Specify --upgrade to force replacement.\u001b[0m\n",
            "\u001b[33mWARNING: Target directory /content/notebooks/pyspark-3.2.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
            "\u001b[33mWARNING: Target directory /content/notebooks/share already exists. Specify --upgrade to force replacement.\u001b[0m\n",
            "\u001b[33mWARNING: Target directory /content/notebooks/bin already exists. Specify --upgrade to force replacement.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark"
      ],
      "metadata": {
        "id": "Sgfarypki8tS"
      },
      "execution_count": 4,
      "outputs": []
    }
  ]
}