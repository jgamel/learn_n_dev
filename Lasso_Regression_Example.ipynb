{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lasso_Regression_Example.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMB0T1W4ylyZCYagsJJVsqo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jgamel/learn_n_dev/blob/python_modeling_forecasting/Lasso_Regression_Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lasso Regression Example"
      ],
      "metadata": {
        "id": "bjcIqOGhoQtW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear regression refers to a model that assumes a linear relationship between input variables and the target variable.\n",
        "\n",
        "With a single input variable, this relationship is a line, and with higher dimensions, this relationship can be thought of as a hyperplane that connects the input variables to the target variable. The coefficients of the model are found via an optimization process that seeks to minimize the sum squared error between the predictions (yhat) and the expected target values (y).\n",
        "\n",
        "A problem with linear regression is that estimated coefficients of the model can become large, making the model sensitive to inputs and possibly unstable. This is particularly true for problems with few observations (samples) or less samples (n) than input predictors (p) or variables (so-called p >> n problems).\n",
        "\n",
        "One approach to address the stability of regression models is to change the loss function to include additional costs for a model that has large coefficients. Linear regression models that use these modified loss functions during training are referred to collectively as penalized linear regression.\n",
        "\n",
        "A popular penalty is to penalize a model based on the sum of the absolute coefficient values. This is called the L1 penalty. An L1 penalty minimizes the size of all coefficients and allows some coefficients to be minimized to the value zero, which removes the predictor from the model.\n",
        "\n",
        "An L1 penalty minimizes the size of all coefficients and allows any coefficient to go to the value of zero, effectively removing input features from the model.\n",
        "\n",
        "This penalty can be added to the cost function for linear regression and is referred to as Least Absolute Shrinkage And Selection Operator regularization (LASSO), or more commonly, “Lasso” (with title case) for short.\n",
        "\n",
        "A hyperparameter is used called “lambda” that controls the weighting of the penalty to the loss function. A default value of 1.0 will give full weightings to the penalty; a value of 0 excludes the penalty. Very small values of lambda, such as 1e-3 or smaller, are common.\n",
        "\n"
      ],
      "metadata": {
        "id": "QwwyIQU1oUTi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example of Lasso Regression"
      ],
      "metadata": {
        "id": "HpGbvECs4QvN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Modules and Data:"
      ],
      "metadata": {
        "id": "OCHXt5Ji4VVH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1bwYoe3n5ZE",
        "outputId": "019311e7-c33f-4d6a-b2ac-88200f8fc9db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(506, 14)\n",
            "        0     1     2   3      4      5     6       7   8      9     10  \\\n",
            "0  0.00632  18.0  2.31   0  0.538  6.575  65.2  4.0900   1  296.0  15.3   \n",
            "1  0.02731   0.0  7.07   0  0.469  6.421  78.9  4.9671   2  242.0  17.8   \n",
            "2  0.02729   0.0  7.07   0  0.469  7.185  61.1  4.9671   2  242.0  17.8   \n",
            "3  0.03237   0.0  2.18   0  0.458  6.998  45.8  6.0622   3  222.0  18.7   \n",
            "4  0.06905   0.0  2.18   0  0.458  7.147  54.2  6.0622   3  222.0  18.7   \n",
            "\n",
            "       11    12    13  \n",
            "0  396.90  4.98  24.0  \n",
            "1  396.90  9.14  21.6  \n",
            "2  392.83  4.03  34.7  \n",
            "3  394.63  2.94  33.4  \n",
            "4  396.90  5.33  36.2  \n"
          ]
        }
      ],
      "source": [
        "# load and summarize the housing dataset\n",
        "from pandas import read_csv\n",
        "from matplotlib import pyplot\n",
        "# load dataset\n",
        "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv'\n",
        "dataframe = read_csv(url, header=None)\n",
        "# summarize shape\n",
        "print(dataframe.shape)\n",
        "# summarize first few lines\n",
        "print(dataframe.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running the example confirms the 506 rows of data and 13 input variables and a single numeric target variable (14 in total). We can also see that all input variables are numeric."
      ],
      "metadata": {
        "id": "pn6RPeBi4mFc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The scikit-learn Python machine learning library provides an implementation of the Lasso penalized regression algorithm via the Lasso class.\n",
        "\n",
        "Confusingly, the lambda term can be configured via the “alpha” argument when defining the class. The default value is 1.0 or a full penalty."
      ],
      "metadata": {
        "id": "Qv_XlxN34o10"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can evaluate the Lasso Regression model on the housing dataset using repeated 10-fold cross-validation and report the average mean absolute error (MAE) on the dataset."
      ],
      "metadata": {
        "id": "Kz1A-TvU5YFT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate an lasso regression model on the dataset\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from numpy import absolute\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.linear_model import Lasso\n",
        "# load the dataset\n",
        "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv'\n",
        "dataframe = read_csv(url, header=None)\n",
        "data = dataframe.values\n",
        "X, y = data[:, :-1], data[:, -1]\n",
        "# define model\n",
        "model = Lasso(alpha=1.0)\n",
        "# define model evaluation method\n",
        "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "# evaluate model\n",
        "scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
        "# force scores to be positive\n",
        "scores = absolute(scores)\n",
        "print('Mean MAE: %.3f (%.3f)' % (mean(scores), std(scores)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vL4r9jZW5cxZ",
        "outputId": "1d9caf7d-60a3-48f8-81d7-6f3acbb07f20"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean MAE: 3.711 (0.549)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running the example evaluates the Lasso Regression algorithm on the housing dataset and reports the average MAE across the three repeats of 10-fold cross-validation.\n",
        "\n",
        "Your specific results may vary given the stochastic nature of the learning algorithm. Consider running the example a few times.\n",
        "\n",
        "In this case, we can see that the model achieved a MAE of about 3.711."
      ],
      "metadata": {
        "id": "9ndrOFes5p0J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We may decide to use the Lasso Regression as our final model and make predictions on new data.\n",
        "\n",
        "This can be achieved by fitting the model on all available data and calling the predict() function, passing in a new row of data.\n",
        "\n",
        "We can demonstrate this with a complete example, listed below."
      ],
      "metadata": {
        "id": "OEYFRHVr5un8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make a prediction with a lasso regression model on the dataset\n",
        "from pandas import read_csv\n",
        "from sklearn.linear_model import Lasso\n",
        "# load the dataset\n",
        "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv'\n",
        "dataframe = read_csv(url, header=None)\n",
        "data = dataframe.values\n",
        "X, y = data[:, :-1], data[:, -1]\n",
        "# define model\n",
        "model = Lasso(alpha=1.0)\n",
        "# fit model\n",
        "model.fit(X, y)\n",
        "# define new data\n",
        "row = [0.00632,18.00,2.310,0,0.5380,6.5750,65.20,4.0900,1,296.0,15.30,396.90,4.98]\n",
        "# make a prediction\n",
        "yhat = model.predict([row])\n",
        "# summarize prediction\n",
        "print('Predicted: %.3f' % yhat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWnooYdR51uI",
        "outputId": "9b583b08-2a38-4cc8-a465-56d7d33f0ace"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: 30.998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running the example fits the model and makes a prediction for the new rows of data.\n",
        "\n",
        "Your specific results may vary given the stochastic nature of the learning algorithm. Try running the example a few times."
      ],
      "metadata": {
        "id": "FEiGkG1rAtuJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tuning Lasso Hyperparameters"
      ],
      "metadata": {
        "id": "r2potYaQA4Ht"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How do we know that the default hyperparameter of alpha=1.0 is appropriate for our dataset?\n",
        "\n",
        "We don’t.\n",
        "\n",
        "Instead, it is good practice to test a suite of different configurations and discover what works best for our dataset.\n",
        "\n",
        "One approach would be to gird search alpha values from perhaps 1e-5 to 100 on a log-10 scale and discover what works best for a dataset. Another approach would be to test values between 0.0 and 1.0 with a grid separation of 0.01. We will try the latter in this case.\n",
        "\n",
        "The example below demonstrates this using the GridSearchCV class with a grid of values we have defined."
      ],
      "metadata": {
        "id": "ymUNQXKIBAop"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# grid search hyperparameters for lasso regression\n",
        "from numpy import arange\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.linear_model import Lasso\n",
        "# load the dataset\n",
        "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv'\n",
        "dataframe = read_csv(url, header=None)\n",
        "data = dataframe.values\n",
        "X, y = data[:, :-1], data[:, -1]\n",
        "# define model\n",
        "model = Lasso()\n",
        "# define model evaluation method\n",
        "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "# define grid\n",
        "grid = dict()\n",
        "grid['alpha'] = arange(0, 1, 0.01)\n",
        "# define search\n",
        "search = GridSearchCV(model, grid, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
        "# perform the search\n",
        "results = search.fit(X, y)\n",
        "# summarize\n",
        "print('MAE: %.3f' % results.best_score_)\n",
        "print('Config: %s' % results.best_params_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q__em-HlBKEg",
        "outputId": "13e0b383-3f76-4444-99da-92a091cf8c47"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE: -3.379\n",
            "Config: {'alpha': 0.01}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running the example will evaluate each combination of configurations using repeated cross-validation.\n",
        "\n",
        "Your specific results may vary given the stochastic nature of the learning algorithm. Try running the example a few times.\n",
        "\n",
        "You might see some warnings that can be safely ignored, such as:"
      ],
      "metadata": {
        "id": "qs_Sghm5BgNq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, we can see that we achieved slightly better results than the default 3.379 vs. 3.711. Ignore the sign; the library makes the MAE negative for optimization purposes.\n",
        "\n",
        "We can see that the model assigned an alpha weight of 0.01 to the penalty."
      ],
      "metadata": {
        "id": "rLhs3XvnBpua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The scikit-learn library also provides a built-in version of the algorithm that automatically finds good hyperparameters via the LassoCV class.\n",
        "\n",
        "To use the class, the model is fit on the training dataset as per normal and the hyperparameters are tuned automatically during the training process. The fit model can then be used to make a prediction.\n",
        "\n",
        "By default, the model will test 100 alpha values. We can change this to a grid of values between 0 and 1 with a separation of 0.01 as we did on the previous example by setting the “alphas” argument.\n",
        "\n",
        "The example below demonstrates this."
      ],
      "metadata": {
        "id": "U7adOvv1BvoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use automatically configured the lasso regression algorithm\n",
        "from numpy import arange\n",
        "from pandas import read_csv\n",
        "from sklearn.linear_model import LassoCV\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "# load the dataset\n",
        "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv'\n",
        "dataframe = read_csv(url, header=None)\n",
        "data = dataframe.values\n",
        "X, y = data[:, :-1], data[:, -1]\n",
        "# define model evaluation method\n",
        "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "# define model\n",
        "model = LassoCV(alphas=arange(0, 1, 0.01), cv=cv, n_jobs=-1)\n",
        "# fit model\n",
        "model.fit(X, y)\n",
        "# summarize chosen configuration\n",
        "print('alpha: %f' % model.alpha_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utBx7TAyBzBK",
        "outputId": "dca8c930-913c-49a9-c1f4-5270b5658caa"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:644: UserWarning: Coordinate descent with alpha=0 may lead to unexpected results and is discouraged.\n",
            "  positive,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:644: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4938.326038547879, tolerance: 3.7370255692307692\n",
            "  positive,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:644: UserWarning: Coordinate descent with alpha=0 may lead to unexpected results and is discouraged.\n",
            "  positive,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:644: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5033.999883655168, tolerance: 3.7981172879120875\n",
            "  positive,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:644: UserWarning: Coordinate descent with alpha=0 may lead to unexpected results and is discouraged.\n",
            "  positive,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:644: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5308.605854251968, tolerance: 3.8821705054945044\n",
            "  positive,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:644: UserWarning: Coordinate descent with alpha=0 may lead to unexpected results and is discouraged.\n",
            "  positive,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:644: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4359.411693485821, tolerance: 3.8221698593406606\n",
            "  positive,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:644: UserWarning: Coordinate descent with alpha=0 may lead to unexpected results and is discouraged.\n",
            "  positive,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:644: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5245.085871826286, tolerance: 3.8665083516483523\n",
            "  positive,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:644: UserWarning: Coordinate descent with alpha=0 may lead to unexpected results and is discouraged.\n",
            "  positive,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:644: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5102.069941107406, tolerance: 3.92876058021978\n",
            "  positive,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:644: UserWarning: Coordinate descent with alpha=0 may lead to unexpected results and is discouraged.\n",
            "  positive,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:644: UserWarning: Coordinate descent with alpha=0 may lead to unexpected results and is discouraged.\n",
            "  positive,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:644: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5122.30292649004, tolerance: 3.9541719649122813\n",
            "  positive,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:644: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4613.39361462729, tolerance: 3.7803832894736846\n",
            "  positive,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:644: UserWarning: Coordinate descent with alpha=0 may lead to unexpected results and is discouraged.\n",
            "  positive,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:644: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4703.960334103987, tolerance: 3.701088938596491\n",
            "  positive,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:644: UserWarning: Coordinate descent with alpha=0 may lead to unexpected results and is discouraged.\n",
            "  positive,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:644: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5211.336901122362, tolerance: 3.966417945175437\n",
            "  positive,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:644: UserWarning: Coordinate descent with alpha=0 may lead to unexpected results and is discouraged.\n",
            "  positive,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:644: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5125.721436586948, tolerance: 3.89033465934066\n",
            "  positive,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:644: UserWarning: Coordinate descent with alpha=0 may lead to unexpected results and is discouraged.\n",
            "  positive,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:644: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4533.389265962076, tolerance: 3.742845230769231\n",
            "  positive,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:644: UserWarning: Coordinate descent with alpha=0 may lead to unexpected results and is discouraged.\n",
            "  positive,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:644: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4820.641174975488, tolerance: 3.766251745054944\n",
            "  positive,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:644: UserWarning: Coordinate descent with alpha=0 may lead to unexpected results and is discouraged.\n",
            "  positive,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:644: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4944.850986194535, tolerance: 3.818408351648351\n",
            "  positive,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:644: UserWarning: Coordinate descent with alpha=0 may lead to unexpected results and is discouraged.\n",
            "  positive,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:644: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5003.353967288185, tolerance: 3.7929101450549454\n",
            "  positive,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:644: UserWarning: Coordinate descent with alpha=0 may lead to unexpected results and is discouraged.\n",
            "  positive,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:644: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5109.406543411715, tolerance: 3.9622208219780224\n",
            "  positive,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:644: UserWarning: Coordinate descent with alpha=0 may lead to unexpected results and is discouraged.\n",
            "  positive,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:644: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4877.199962414157, tolerance: 3.621472421052632\n",
            "  positive,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:644: UserWarning: Coordinate descent with alpha=0 may lead to unexpected results and is discouraged.\n",
            "  positive,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:644: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4824.118580147013, tolerance: 3.8953458925438604\n",
            "  positive,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:644: UserWarning: Coordinate descent with alpha=0 may lead to unexpected results and is discouraged.\n",
            "  positive,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:644: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5083.671343099508, tolerance: 3.988871570175439\n",
            "  positive,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:644: UserWarning: Coordinate descent with alpha=0 may lead to unexpected results and is discouraged.\n",
            "  positive,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:644: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5246.023402267754, tolerance: 3.9532313662280716\n",
            "  positive,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:644: UserWarning: Coordinate descent with alpha=0 may lead to unexpected results and is discouraged.\n",
            "  positive,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:644: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5049.8328847049415, tolerance: 3.8058872835164834\n",
            "  positive,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:644: UserWarning: Coordinate descent with alpha=0 may lead to unexpected results and is discouraged.\n",
            "  positive,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:644: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5199.897867372583, tolerance: 4.005465749450549\n",
            "  positive,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:644: UserWarning: Coordinate descent with alpha=0 may lead to unexpected results and is discouraged.\n",
            "  positive,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:644: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5042.312198197815, tolerance: 3.7652059648351637\n",
            "  positive,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:644: UserWarning: Coordinate descent with alpha=0 may lead to unexpected results and is discouraged.\n",
            "  positive,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:644: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4942.764618866218, tolerance: 3.8348627912087907\n",
            "  positive,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:644: UserWarning: Coordinate descent with alpha=0 may lead to unexpected results and is discouraged.\n",
            "  positive,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:644: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4653.8262606935605, tolerance: 3.587134936263736\n",
            "  positive,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:644: UserWarning: Coordinate descent with alpha=0 may lead to unexpected results and is discouraged.\n",
            "  positive,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:644: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4758.911576524377, tolerance: 3.783493437362637\n",
            "  positive,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "alpha: 0.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:644: UserWarning: Coordinate descent with alpha=0 may lead to unexpected results and is discouraged.\n",
            "  positive,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:644: UserWarning: Coordinate descent with alpha=0 may lead to unexpected results and is discouraged.\n",
            "  positive,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:644: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4748.899934575311, tolerance: 3.9281246228070175\n",
            "  positive,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:644: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4955.416755393628, tolerance: 3.8868426644736846\n",
            "  positive,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:644: UserWarning: Coordinate descent with alpha=0 may lead to unexpected results and is discouraged.\n",
            "  positive,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:644: UserWarning: Coordinate descent with alpha=0 may lead to unexpected results and is discouraged.\n",
            "  positive,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:644: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5152.6502864264185, tolerance: 3.9961889451754375\n",
            "  positive,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:644: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5167.833209657409, tolerance: 3.84503599122807\n",
            "  positive,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:1714: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
            "  model.fit(X, y)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.539e+03, tolerance: 4.272e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running the example fits the model and discovers the hyperparameters that give the best results using cross-validation.\n",
        "\n",
        "Your specific results may vary given the stochastic nature of the learning algorithm. Try running the example a few times.\n",
        "\n",
        "In this case, we can see that the model chose the hyperparameter of alpha=0.0. This is different from what we found via our manual grid search, perhaps due to the systematic way in which configurations were searched or selected."
      ],
      "metadata": {
        "id": "nscvdbqjCDMN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary"
      ],
      "metadata": {
        "id": "2fmBYHlxCMrf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " In this tutorial, you discovered how to develop and evaluate Lasso Regression models in Python.\n",
        "\n",
        "Specifically, you learned:\n",
        "\n",
        "* Lasso Regression is an extension of linear regression that adds a regularization penalty to the loss function during training.\n",
        "* How to evaluate a Lasso Regression model and use a final model to make predictions for new data.\n",
        "* How to configure the Lasso Regression model for a new dataset via grid search and automatically."
      ],
      "metadata": {
        "id": "O0jABjQtCOPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jJBz7pbX5qtl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}