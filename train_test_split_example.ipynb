{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_test_split_example.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "GKGY8i7j64Yt",
        "k-1Uwhh9xotu",
        "ujDywW3U3Bie",
        "Bm_S8oCgP2ew",
        "GhwZrXjeVMd9"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jgamel/learn_n_dev/blob/python_ds_examples/train_test_split_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Application of train_test_split()"
      ],
      "metadata": {
        "id": "KaDBtneMxDla"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Import packages and classes"
      ],
      "metadata": {
        "id": "76t2QUGMxJ5I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NrV5bN6-wMXE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that you have both imported, you can use them to split data into training sets and test sets. You’ll split inputs and outputs at the same time, with a single function call.\n",
        "\n",
        "With train_test_split(), you need to provide the sequences that you want to split as well as any optional arguments. It returns a list of NumPy arrays, other sequences, or SciPy sparse matrices if appropriate:"
      ],
      "metadata": {
        "id": "xNTn8178yGx_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "arrays is the sequence of lists, NumPy arrays, pandas DataFrames, or similar array-like objects that hold the data you want to split. All these objects together make up the dataset and must be of the same length.\n",
        "\n",
        "In supervised machine learning applications, you’ll typically work with two such sequences:\n",
        "\n",
        "A two-dimensional array with the inputs (x)\n",
        "A one-dimensional array with the outputs (y)\n",
        "options are the optional keyword arguments that you can use to get desired behavior:\n",
        "\n",
        "train_size is the number that defines the size of the training set. If you provide a float, then it must be between 0.0 and 1.0 and will define the share of the dataset used for testing. If you provide an int, then it will represent the total number of the training samples. The default value is None.\n",
        "\n",
        "test_size is the number that defines the size of the test set. It’s very similar to train_size. You should provide either train_size or test_size. If neither is given, then the default share of the dataset that will be used for testing is 0.25, or 25 percent.\n",
        "\n",
        "random_state is the object that controls randomization during splitting. It can be either an int or an instance of RandomState. The default value is None.\n",
        "\n",
        "shuffle is the Boolean object (True by default) that determines whether to shuffle the dataset before applying the split.\n",
        "\n",
        "stratify is an array-like object that, if not None, determines how to use a stratified split.\n",
        "\n",
        "Now it’s time to try data splitting! You’ll start by creating a simple dataset to work with. The dataset will contain the inputs in the two-dimensional array x and outputs in the one-dimensional array y:"
      ],
      "metadata": {
        "id": "fIUWujgLyHlv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Get Data"
      ],
      "metadata": {
        "id": "BQoKAR2cyd2Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.arange(1, 25).reshape(12, 2)\n",
        "\n",
        "y = np.array([0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0])"
      ],
      "metadata": {
        "id": "Tng_6VsjyMj0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxrfamBvyzyo",
        "outputId": "e502beb1-184c-4eb1-c87f-3dda4ca080d7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1,  2],\n",
              "       [ 3,  4],\n",
              "       [ 5,  6],\n",
              "       [ 7,  8],\n",
              "       [ 9, 10],\n",
              "       [11, 12],\n",
              "       [13, 14],\n",
              "       [15, 16],\n",
              "       [17, 18],\n",
              "       [19, 20],\n",
              "       [21, 22],\n",
              "       [23, 24]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKxJFAcvy1lH",
        "outputId": "4c4cb5f1-4399-44af-a5cd-77829df47186"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get your data, you use arange(), which is very convenient for generating arrays based on numerical ranges. You also use .reshape() to modify the shape of the array returned by arange() and get a two-dimensional data structure.\n",
        "\n",
        "You can split both input and output datasets with a single function call:"
      ],
      "metadata": {
        "id": "Jw6LCxmzy9Zj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Split Data"
      ],
      "metadata": {
        "id": "19dW-QH9zdZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x, y)\n",
        "x_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJCP0YFizmuM",
        "outputId": "e8504c25-cfeb-4847-da95-31a8ed829809"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 3,  4],\n",
              "       [17, 18],\n",
              "       [ 1,  2],\n",
              "       [15, 16],\n",
              "       [ 5,  6],\n",
              "       [ 7,  8],\n",
              "       [ 9, 10],\n",
              "       [19, 20],\n",
              "       [13, 14]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_test, end='\\n\\n')\n",
        "\n",
        "print(y_train, end='\\n\\n')\n",
        "\n",
        "print(y_test, end='\\n\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_3xnGjZ0fRe",
        "outputId": "2b485ba1-2b87-4d8f-d745-93cccfc2db6c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[23 24]\n",
            " [21 22]\n",
            " [11 12]]\n",
            "\n",
            "[1 1 0 1 1 0 1 0 0]\n",
            "\n",
            "[0 1 0]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given two sequences, like x and y here, train_test_split() performs the split and returns four sequences (in this case NumPy arrays) in this order:\n",
        "\n",
        "x_train: The training part of the first sequence (x)\n",
        "x_test: The test part of the first sequence (x)\n",
        "y_train: The training part of the second sequence (y)\n",
        "y_test: The test part of the second sequence (y)\n",
        "You probably got different results from what you see here. This is because dataset splitting is random by default. The result differs each time you run the function. However, this often isn’t what you want.\n",
        "\n",
        "Sometimes, to make your tests reproducible, you need a random split with the same output for each function call. You can do that with the parameter random_state. The value of random_state isn’t important—it can be any non-negative integer. You could use an instance of numpy.random.RandomState instead, but that is a more complex approach.\n",
        "\n",
        "In the previous example, you used a dataset with twelve observations (rows) and got a training sample with nine rows and a test sample with three rows. That’s because you didn’t specify the desired size of the training and test sets. By default, 25 percent of samples are assigned to the test set. This ratio is generally fine for many applications, but it’s not always what you need.\n",
        "\n",
        "Typically, you’ll want to define the size of the test (or training) set explicitly, and sometimes you’ll even want to experiment with different values. You can do that with the parameters train_size or test_size.\n",
        "\n",
        "Modify the code so you can choose the size of the test set and get a reproducible result:"
      ],
      "metadata": {
        "id": "vmxPqcls1nP9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "     x, y, test_size=4, random_state=4\n",
        " )\n",
        "print (x_train, end='\\n\\n')\n",
        "\n",
        "print (x_test, end='\\n\\n')\n",
        "\n",
        "print (y_train, end='\\n\\n')\n",
        "\n",
        "print (y_test, end='\\n\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvxtKpK61oq0",
        "outputId": "82aa9261-6d3f-4491-c75f-3d9c847e93de"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[17 18]\n",
            " [ 5  6]\n",
            " [23 24]\n",
            " [ 1  2]\n",
            " [ 3  4]\n",
            " [11 12]\n",
            " [15 16]\n",
            " [21 22]]\n",
            "\n",
            "[[ 7  8]\n",
            " [ 9 10]\n",
            " [13 14]\n",
            " [19 20]]\n",
            "\n",
            "[1 1 0 0 1 0 1 1]\n",
            "\n",
            "[0 1 0 0]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With this change, you get a different result from before. Earlier, you had a training set with nine items and test set with three items. Now, thanks to the argument test_size=4, the training set has eight items and the test set has four items. You’d get the same result with test_size=0.33 because 33 percent of twelve is approximately four.\n",
        "\n",
        "There’s one more very important difference between the last two examples: You now get the same result each time you run the function. This is because you’ve fixed the random number generator with random_state=4."
      ],
      "metadata": {
        "id": "ZPYRyhBX6eCY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stratified Sample\n",
        "\n",
        "If you want to (approximately) keep the proportion of y values through the training and test sets, then pass stratify=y. This will enable stratified splitting:"
      ],
      "metadata": {
        "id": "GKGY8i7j64Yt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "     x, y, test_size=0.33, random_state=4, stratify=y\n",
        " )\n"
      ],
      "metadata": {
        "id": "sbcPV_To7H5l"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pl-QlAMG7Nzv",
        "outputId": "84316aee-5f42-4d2c-9e34-2bcc71400733"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[21, 22],\n",
              "       [ 1,  2],\n",
              "       [15, 16],\n",
              "       [13, 14],\n",
              "       [17, 18],\n",
              "       [19, 20],\n",
              "       [23, 24],\n",
              "       [ 3,  4]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpdnDsm_7Smx",
        "outputId": "d456a882-953e-445e-ad05-a733e5304e46"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[11, 12],\n",
              "       [ 7,  8],\n",
              "       [ 5,  6],\n",
              "       [ 9, 10]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGIeeGf07aG8",
        "outputId": "8a311a54-332a-44ba-9ae9-6bb28796773d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 1, 0, 1, 0, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUBNvDEB7e33",
        "outputId": "cef9db98-37e2-402c-b48d-5d34bc11c383"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now y_train and y_test have the same ratio of zeros and ones as the original y array.\n",
        "\n",
        "Stratified splits are desirable in some cases, like when you’re classifying an imbalanced dataset, a dataset with a significant difference in the number of samples that belong to distinct classes.\n",
        "\n",
        "Finally, you can turn off data shuffling and random split with shuffle=False:"
      ],
      "metadata": {
        "id": "Ha40mSFz7lhW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "     x, y, test_size=0.33, shuffle=False\n",
        " )"
      ],
      "metadata": {
        "id": "Lqw-bb-i-O5s"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqPgvVKP-VRc",
        "outputId": "ac0ff9d6-acc5-4b49-f8bb-6d9f63803841"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1,  2],\n",
              "       [ 3,  4],\n",
              "       [ 5,  6],\n",
              "       [ 7,  8],\n",
              "       [ 9, 10],\n",
              "       [11, 12],\n",
              "       [13, 14],\n",
              "       [15, 16]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BV2-ZUui-aV7",
        "outputId": "15eb672a-c917-4871-ffcc-edbbaccfa969"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[17, 18],\n",
              "       [19, 20],\n",
              "       [21, 22],\n",
              "       [23, 24]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKp-z-UM-eY5",
        "outputId": "2d7070e1-ba7b-4de0-8103-1cefc430cbf5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 1, 0, 1, 0, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vNQd678-iJf",
        "outputId": "282d37bf-a1a2-4c19-9fb3-af481e6ff7ff"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 1, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you have a split in which the first two-thirds of samples in the original x and y arrays are assigned to the training set and the last third to the test set. No shuffling. No randomness."
      ],
      "metadata": {
        "id": "AYDJR0Pt-r-y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Supervised Machine Learning With train_test_split()\n",
        "\n",
        "Now it’s time to see train_test_split() in action when solving supervised learning problems. You’ll start with a small regression problem that can be solved with linear regression before looking at a bigger problem. You’ll also see that you can use train_test_split() for classification as well.\n",
        "\n",
        "Minimalist Example of Linear Regression\n",
        "In this example, you’ll apply what you’ve learned so far to solve a small regression problem. You’ll learn how to create datasets, split them into training and test subsets, and use them for linear regression.\n",
        "\n",
        "As always, you’ll start by importing the necessary packages, functions, or classes. You’ll need NumPy, LinearRegression, and train_test_split():"
      ],
      "metadata": {
        "id": "5dftsJDD-scc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "-_J0YEs7_Dwy"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that you’ve imported everything you need, you can create two small arrays, x and y, to represent the observations and then split them into training and test sets just as you did before:"
      ],
      "metadata": {
        "id": "ZxsHiGdLHs7M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.arange(20).reshape(-1, 1)\n",
        "y = np.array([5, 12, 11, 19, 30, 29, 23, 40, 51, 54, 74,\n",
        "               62, 68, 73, 89, 84, 89, 101, 99, 106])\n",
        "print(x, end='\\n\\n')\n",
        "\n",
        "print(y, end='\\n\\n')\n",
        "\n",
        "\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "     x, y, test_size=8, random_state=0\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoXPwUHLHt8E",
        "outputId": "56a5d0ec-dd69-48a6-d439-e0b8243251c2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0]\n",
            " [ 1]\n",
            " [ 2]\n",
            " [ 3]\n",
            " [ 4]\n",
            " [ 5]\n",
            " [ 6]\n",
            " [ 7]\n",
            " [ 8]\n",
            " [ 9]\n",
            " [10]\n",
            " [11]\n",
            " [12]\n",
            " [13]\n",
            " [14]\n",
            " [15]\n",
            " [16]\n",
            " [17]\n",
            " [18]\n",
            " [19]]\n",
            "\n",
            "[  5  12  11  19  30  29  23  40  51  54  74  62  68  73  89  84  89 101\n",
            "  99 106]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your dataset has twenty observations, or x-y pairs. You specify the argument test_size=8, so the dataset is divided into a training set with twelve observations and a test set with eight observations.\n",
        "\n",
        "Now you can use the training set to fit the model:"
      ],
      "metadata": {
        "id": "zvp-LXxwJrbQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = LinearRegression().fit(x_train, y_train)\n",
        "model.intercept_\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFYfeK-QKbcc",
        "outputId": "0cbd8a52-5858-4d1f-bcfd-220843301585"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.1617195496417523"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.coef_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvx-FzszKf1r",
        "outputId": "6d2465dc-1f5f-4b62-f974-b8c3be1cd252"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5.53121801])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LinearRegression creates the object that represents the model, while .fit() trains, or fits, the model and returns it. With linear regression, fitting the model means determining the best intercept (model.intercept_) and slope (model.coef_) values of the regression line.\n",
        "\n",
        "Although you can use x_train and y_train to check the goodness of fit, this isn’t a best practice. An unbiased estimation of the predictive performance of your model is based on test data:"
      ],
      "metadata": {
        "id": "F5psmPQ6Lp45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.score(x_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCkOpdNfLw6n",
        "outputId": "6f8bcf1c-b116-496b-a56a-196779d8eb86"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9868175024574795"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.score(x_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihiGZCiYL1SF",
        "outputId": "6f4515ef-3a5a-4d5c-fd93-4838486c9642"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9465896927715023"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ".score() returns the coefficient of determination, or R², for the data passed. Its maximum is 1. The higher the R² value, the better the fit. In this case, the training data yields a slightly higher coefficient. However, the R² calculated with test data is an unbiased measure of your model’s prediction performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "4Yo5LOGQmH0d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regression Example"
      ],
      "metadata": {
        "id": "IKK8J3I3hWPB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you’re ready to split a larger dataset to solve a regression problem. You’ll use a well-known [Boston house prices](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html#sklearn.datasets.load_boston) dataset, which is included in sklearn. This dataset has 506 samples, 13 input variables, and the house values as the output. You can retrieve it with load_boston().\n",
        "\n",
        "First, import train_test_split() and load_boston():\n"
      ],
      "metadata": {
        "id": "tfXUMgRzhexS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_boston\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "50toaU1aiZ2G"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that you have both functions imported, you can get the data to work with:"
      ],
      "metadata": {
        "id": "8Zvq_9ueihky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = load_boston(return_X_y=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfXCXniPiin1",
        "outputId": "74814ad2-54e5-48bb-9622-c8fa91f95dec"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
            "\n",
            "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
            "    the documentation of this function for further details.\n",
            "\n",
            "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
            "    dataset unless the purpose of the code is to study and educate about\n",
            "    ethical issues in data science and machine learning.\n",
            "\n",
            "    In this special case, you can fetch the dataset from the original\n",
            "    source::\n",
            "\n",
            "        import pandas as pd\n",
            "        import numpy as np\n",
            "\n",
            "\n",
            "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
            "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
            "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
            "        target = raw_df.values[1::2, 2]\n",
            "\n",
            "    Alternative datasets include the California housing dataset (i.e.\n",
            "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
            "    dataset. You can load the datasets as follows::\n",
            "\n",
            "        from sklearn.datasets import fetch_california_housing\n",
            "        housing = fetch_california_housing()\n",
            "\n",
            "    for the California housing dataset and::\n",
            "\n",
            "        from sklearn.datasets import fetch_openml\n",
            "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
            "\n",
            "    for the Ames housing dataset.\n",
            "    \n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, load_boston() with the argument return_X_y=True returns a tuple with two NumPy arrays:\n",
        "\n",
        "A two-dimensional array with the inputs\n",
        "A one-dimensional array with the outputs\n",
        "The next step is to split the data the same way as before:"
      ],
      "metadata": {
        "id": "vsV4LAZJiufV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "     x, y, test_size=0.4, random_state=0\n",
        " )"
      ],
      "metadata": {
        "id": "hwdJJ0fRivQL"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you have the training and test sets. The training data is contained in x_train and y_train, while the data for testing is in x_test and y_test.\n",
        "\n",
        "When you work with larger datasets, it’s usually more convenient to pass the training or test size as a ratio. test_size=0.4 means that approximately 40 percent of samples will be assigned to the test data, and the remaining 60 percent will be assigned to the training data.\n",
        "\n",
        "Finally, you can use the training set (x_train and y_train) to fit the model and the test set (x_test and y_test) for an unbiased evaluation of the model. In this example, you’ll apply three well-known regression algorithms to create models that fit your data:\n",
        "\n",
        "Linear regression with LinearRegression()\n",
        "\n",
        "[Gradient boosting](https://en.wikipedia.org/wiki/Gradient_boosting) with [GradientBoostingRegressor()](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html)\n",
        "\n",
        "Random forest with RandomForestRegressor()\n"
      ],
      "metadata": {
        "id": "H31n-ZRsjZuJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The process is pretty much the same as with the previous example:\n",
        "\n",
        "Import the classes you need.\n",
        "Create model instances using these classes.\n",
        "Fit the model instances with .fit() using the training set.\n",
        "Evaluate the model with .score() using the test set.\n",
        "Here’s the code that follows the steps described above for all three regression algorithms:"
      ],
      "metadata": {
        "id": "yAc4wrrlxm4X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear Regression"
      ],
      "metadata": {
        "id": "k-1Uwhh9xotu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "model = LinearRegression().fit(x_train, y_train)\n"
      ],
      "metadata": {
        "id": "mU1k-xQEy5qA"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.score(x_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPhNdG5JzWs_",
        "outputId": "975b044a-3fad-4a5d-d036-d608f48d3166"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7668160223286261"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.score(x_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZR6VvPIzaJW",
        "outputId": "38371cda-2557-48e3-88bb-b42edf2e8cb2"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.688260714253802"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradient Boosting"
      ],
      "metadata": {
        "id": "ujDywW3U3Bie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "model = GradientBoostingRegressor(random_state=0).fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "-zCqiw0G_3ho"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.score(x_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hg7Vir6DAArn",
        "outputId": "5a56facb-d5ca-4c45-a856-2912fee312d3"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9859065238883613"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.score(x_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlEdNbdAAEAW",
        "outputId": "57677672-5b81-4d61-b692-563a52c96e54"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8530127436482149"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forest"
      ],
      "metadata": {
        "id": "Bm_S8oCgP2ew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "model = RandomForestRegressor(random_state=0).fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "KCky4yQEP6av"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.score(x_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ueoHtKXPQDW5",
        "outputId": "2e16cc1e-b6bb-492c-9980-b6bb85eb2511"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9811695664860354"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.score(x_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbiSh57FQJ8L",
        "outputId": "720528c2-bb8c-43f1-8675-75efdf4cbc95"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8325867908704008"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You’ve used your training and test datasets to fit three models and evaluate their performance. The measure of accuracy obtained with .score() is the coefficient of determination. It can be calculated with either the training or test set. However, as you already learned, the score obtained with the test set represents an unbiased estimation of performance.\n",
        "\n",
        "As mentioned in the documentation, you can provide optional arguments to LinearRegression(), GradientBoostingRegressor(), and RandomForestRegressor(). GradientBoostingRegressor() and RandomForestRegressor() use the random_state parameter for the same reason that train_test_split() does: to deal with randomness in the algorithms and ensure reproducibility."
      ],
      "metadata": {
        "id": "6nDwN8tLQRwT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification Example"
      ],
      "metadata": {
        "id": "W65ruA-EUkUe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can use train_test_split() to solve classification problems the same way you do for regression analysis. In machine learning, classification problems involve training a model to apply labels to, or classify, the input values and sort your dataset into categories.\n",
        "\n",
        "In the tutorial [Logistic Regression](https://realpython.com/logistic-regression-python/) in Python, you’ll find an example of a [handwriting recognition](https://realpython.com/logistic-regression-python/#logistic-regression-in-python-handwriting-recognition) task. The example provides another demonstration of splitting data into training and test sets to avoid bias in the evaluation process."
      ],
      "metadata": {
        "id": "2HV1RFGRUloA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Other Validation Functionalities"
      ],
      "metadata": {
        "id": "GhwZrXjeVMd9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The package sklearn.model_selection offers a lot of functionalities related to model selection and validation, including the following:\n",
        "\n",
        "Cross-validation\n",
        "\n",
        "Learning curves\n",
        "\n",
        "Hyperparameter tuning\n",
        "\n",
        "[Cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)) is a set of techniques that combine the measures of prediction performance to get more accurate model estimations.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "One of the widely used cross-validation methods is [k-fold cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)#k-fold_cross-validation). In it, you divide your dataset into k (often five or ten) subsets, or folds, of equal size and then perform the training and test procedures k times. Each time, you use a different fold as the test set and all the remaining folds as the training set. This provides k measures of predictive performance, and you can then analyze their mean and standard deviation.\n",
        "\n",
        "You can implement cross-validation with [KFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html), [StratifiedKFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html), [LeaveOneOut](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeaveOneOut.html), and a few other classes and functions from sklearn.model_selection.\n",
        "\n",
        "A [learning curve](https://en.wikipedia.org/wiki/Learning_curve_(machine_learning)), sometimes called a training curve, shows how the prediction score of training and validation sets depends on the number of training samples. You can use [learning_curve()](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.learning_curve.html) to get this dependency, which can help you find the optimal size of the training set, choose hyperparameters, compare models, and so on.\n",
        "\n",
        "[Hyperparameter tuning](https://en.wikipedia.org/wiki/Hyperparameter_optimization), also called hyperparameter optimization, is the process of determining the best set of hyperparameters to define your machine learning model. sklearn.model_selection provides you with several options for this purpose, including [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html), [RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html), [validation_curve()](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.validation_curve.html), and others. Splitting your data is also important for hyperparameter tuning."
      ],
      "metadata": {
        "id": "X9sFDAX5WUJX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conclusion\n",
        "You now know why and how to use train_test_split() from sklearn. You’ve learned that, for an unbiased estimation of the predictive performance of machine learning models, you should use data that hasn’t been used for model fitting. That’s why you need to split your dataset into training, test, and in some cases, validation subsets.\n",
        "\n",
        "In this tutorial, you’ve learned how to:\n",
        "\n",
        "Use train_test_split() to get training and test sets\n",
        "Control the size of the subsets with the parameters train_size and test_size\n",
        "Determine the randomness of your splits with the random_state parameter\n",
        "Obtain stratified splits with the stratify parameter\n",
        "Use train_test_split() as a part of supervised machine learning procedures\n"
      ],
      "metadata": {
        "id": "T0W_SWAvC2Ow"
      }
    }
  ]
}