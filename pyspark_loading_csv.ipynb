{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pyspark_loading_csv.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOMFvAFi8Fd7Bs65yMh1SGy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jgamel/learn_n_dev/blob/PySpark/pyspark_loading_csv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PySpark on Google Colab Load CSV Example"
      ],
      "metadata": {
        "id": "hdfkEDHKwcFM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following features makes Apache Spark more unique,\n",
        "\n",
        "* Speed — Run workloads 100x faster.\n",
        "* Ease of Use — Open for several programming languages such as Java, Scala, Python, and R.\n",
        "* Generality — Combine SQL, streaming, and complex analytics.\n",
        "* Runs Everywhere — Runs on Hadoop, Apache Mesos, Kubernetes, standalone, or in the cloud."
      ],
      "metadata": {
        "id": "pvcHUTWvxAKP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting Up Spark on Colabs"
      ],
      "metadata": {
        "id": "d4HTz0XBxMK2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up PySpark in Colab\n",
        "\n",
        "Spark is written in the Scala programming language and requires the Java Virtual Machine (JVM) to run. Therefore, our first task is to download Java."
      ],
      "metadata": {
        "id": "TXJcqbcvx-3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "metadata": {
        "id": "A5_5G8H2x_8G"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will download and unzip Apache Spark with Hadoop 2.7 to install it.\n",
        "\n",
        "Apache Spark Versions:\n",
        "http://apache.osuosl.org/spark/"
      ],
      "metadata": {
        "id": "tzPDvGlYyWRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q http://apache.osuosl.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop2.7.tgz \n",
        "!tar xf spark-3.2.1-bin-hadoop2.7.tgz"
      ],
      "metadata": {
        "id": "1RSGrClhyXEL"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, it’s time to set the ‘environment’ path."
      ],
      "metadata": {
        "id": "-sBVqAqwyuWD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"spark-3.2.1-bin-hadoop2.7/\""
      ],
      "metadata": {
        "id": "LF4aE8NdyxQi"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we need to import the ‘findspark’ library that will locate Spark on the system and import it as a regular library."
      ],
      "metadata": {
        "id": "TniTWwL-y26J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install findspark"
      ],
      "metadata": {
        "id": "OhUhbeeWcl0u",
        "outputId": "ed2b99e7-d4bc-4659-d85f-618d080543d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: findspark in /usr/local/lib/python3.7/dist-packages (2.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "en2RSRZUy3gp"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can import SparkSession from pyspark.sql and create a SparkSession, which is the entry point to Spark."
      ],
      "metadata": {
        "id": "ppKwgRFFzAVZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()\n",
        "\n"
      ],
      "metadata": {
        "id": "xK7WkXDmzo_l"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkFiles\n",
        "url = \"https://raw.githubusercontent.com/jgamel/learn_n_dev/input_data/Mall_Customers.csv\"\n",
        "spark.sparkContext.addFile(url)"
      ],
      "metadata": {
        "id": "RAF2LOPNl7dn"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "id": "uxUUayVfzMm9",
        "outputId": "4ac1fceb-b571-4b64-9d3b-8a2aa1c367c7"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f2b3edb5350>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://a506d70be193:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.2.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Colab</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading data into PySpark\n",
        "\n",
        "In PySpark we deal with large-scale datasets. So it’s an important task to load data for data processing. The following command shows how to load data into PySpark. Here we are using a simple data set that contains customer data. In read.csv() we have pass two parameters which are the path of our CSV file and header=True for accepting the header of our CSV file."
      ],
      "metadata": {
        "id": "16G2a86rzOKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.csv(SparkFiles.get(\"Mall_Customers.csv\"), header=True, inferSchema= True)"
      ],
      "metadata": {
        "id": "K7ErvLNYmXEY"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Exploration with PySpark DF\n",
        "\n",
        "After loading data, we can perform several tasks related to our dataset. Let’s explore a few of them.\n",
        "\n",
        "* Display data - By show() operator we can display our dataset as follows."
      ],
      "metadata": {
        "id": "7qYMBOkz0WfP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KTXIM2z0cNa",
        "outputId": "86bd94db-7c9e-420d-eeb5-da3f9d82cee9"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------+---+------------------+----------------------+\n",
            "|CustomerID|Gender|Age|Annual Income (k$)|Spending Score (1-100)|\n",
            "+----------+------+---+------------------+----------------------+\n",
            "|         1|  Male| 19|                15|                    39|\n",
            "|         2|  Male| 21|                15|                    81|\n",
            "|         3|Female| 20|                16|                     6|\n",
            "|         4|Female| 23|                16|                    77|\n",
            "|         5|Female| 31|                17|                    40|\n",
            "|         6|Female| 22|                17|                    76|\n",
            "|         7|Female| 35|                18|                     6|\n",
            "|         8|Female| 23|                18|                    94|\n",
            "|         9|  Male| 64|                19|                     3|\n",
            "|        10|Female| 30|                19|                    72|\n",
            "+----------+------+---+------------------+----------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Count the records"
      ],
      "metadata": {
        "id": "zzDpeRiU0nBs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOkuf2qt0psG",
        "outputId": "7e5756af-c1c5-43de-a992-d51151f8f56b"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "200"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Drop null values - If there are any null values on the dataset remove them."
      ],
      "metadata": {
        "id": "9NcON8jx0uJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.na.drop()\n",
        "df.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eoM6uDzn1DpB",
        "outputId": "b6484524-c78a-4408-d95c-77968d608ed7"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "200"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Display specific columns only"
      ],
      "metadata": {
        "id": "0UhlWy4S1Nsw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(\"Gender\",\"Age\").show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHaHg17-1Q4-",
        "outputId": "4695dae0-3ef9-4664-82a7-724bd189ff98"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---+\n",
            "|Gender|Age|\n",
            "+------+---+\n",
            "|  Male| 19|\n",
            "|  Male| 21|\n",
            "|Female| 20|\n",
            "|Female| 23|\n",
            "|Female| 31|\n",
            "+------+---+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    }
  ]
}